{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrAhK4QF3jSd",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install Unsloth's specific dependencies\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "\n",
        "# Install Unsloth\n",
        "!pip install --no-deps unsloth\n",
        "\n",
        "# Pin TRL to a specific compatible version\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This list contains all 18 configurations for your training loop.\n",
        "all_chapter_configs = [\n",
        "    {\n",
        "        \"chapter_num\": 1,\n",
        "        \"csv_file\": \"Chapter_1_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"A specialist in articulating and validating complex human dilemmas, grief, and moral confusion. This LLM is the \"empathy engine.\" It's fine-tuned on the state of conflict (dharma-sankata) and the physical and emotional symptoms of profound despair, serving as the user's first point of contact to understand their problem.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 2,\n",
        "        \"csv_file\": \"Chapter_2_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"The core \"knowledge\" expert. This LLM is an expert in defining the immortal nature of the self (Atman) versus the ephemeral body and emotions. Its key function is to explain the qualities of a Sthitaprajna-the person with a steady, unshakable mind- and to re-frame problems in the context of eternal truth.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 3,\n",
        "        \"csv_file\": \"Chapter_3_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"A specialist in the \"why\" and \"how\" of selfless action (Karma Yoga). It's an expert on productivity without attachment to the outcome. It also functions as a diagnostic tool, identifying internal \"blockers\" like selfish desire (kama) and anger (krodha) that corrupt action.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 4,\n",
        "        \"csv_file\": \"Chapter_4_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"This LLM is an expert on the synergy between knowledge and action. It explains the \"operating system\" of yajna (seeing all action as an offering or sacrifice) and the lineage of wisdom. It clarifies how enlightened action is possible and why it's superior to inaction.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 5,\n",
        "        \"csv_file\": \"Chapter_5_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"An expert in the nuanced philosophy of \"renunciation in action.\" Its specialty is distinguishing between renouncing actions (fleeing the world) and renouncing the fruits of action (acting in the world). Its prime directive is to describe the state of \"engaged detachment,\" like a lotus leaf on water.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 6,\n",
        "        \"csv_file\": \"Chapter_6_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"A practical guide for meditation and mental control (Dhyana Yoga). It provides step-by-step techniques for posture, focus, and taming the \"restless mind.\" It's an expert at diagnosing mental fluctuations and prescribing specific contemplative methods to achieve inner stability.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 7,\n",
        "        \"csv_file\": \"Chapter_7_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"An expert in the underlying metaphysics of reality. It defines the divine \"source code,\" differentiating between the material (Apara Prakriti) and spiritual (Para Prakriti) energies that form the universe. It's also the specialist on Maya (illusion) and how it deludes.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 8,\n",
        "        \"csv_file\": \"Chapter_8_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"A specialist in the metaphysics of transition. Its expertise covers the critical moment of death, the process of the soul's journey, and the cosmic cycles of creation and dissolution. It answers the question, \"What happens next?\" \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 9,\n",
        "        \"csv_file\": \"Chapter_9_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"An expert on the \"royal secret\" of immanence. It explains how the divine pervades all of creation while remaining impartial and unattached. It champions the power of simple, direct devotion and explains how God is the \"recipient\" of all sincere offerings.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 10,\n",
        "        \"csv_file\": \"Chapter_10_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"A \"pattern recognition\" expert for finding the divine in the mundane. It's trained on the \"Vibhutis\" (divine opulences) and is an expert at identifying the \"best of\" all categories (\"I am the sun among lights,\" \"the lion among beasts\") to inspire awe and a devotional perspective on the world.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 11,\n",
        "        \"csv_file\": \"Chapter_11_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"The \"big picture\" processor. This LLM is fine-tuned on the Universal Form (Vishvarupa). It's an expert in synthesizing the overwhelming, terrifying, and awesome vision of reality where all of time and space, creation and destruction, are happening at once.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 12,\n",
        "        \"csv_file\": \"Chapter_12_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"The expert on the path of devotion (Bhakti Yoga). It specializes in defining the qualities of the ideal devotee: compassion, contentment, equanimity, and freedom from envy. It champions the personal, loving path to the divine as the most direct.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 13,\n",
        "        \"csv_file\": \"Chapter_13_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"A high-precision analytical tool. Its core function is to surgically distinguish between the \"Field\" (Kshetra - the body, matter, emotions) and the \"Knower of the Field\" (Kshetragna - the soul, consciousness). It helps the user develop the power of discernment.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 14,\n",
        "        \"csv_file\": \"Chapter_14_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"A diagnostic engine. Its entire function is to identify and explain the three Gunas (Sattva, Rajas, Tamas). It analyzes how these three fundamental qualities of nature influence a person's thoughts, actions, and attachments, effectively diagnosing the \"root cause\" of their mental state.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 15,\n",
        "        \"csv_file\": \"Chapter_15_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"A metaphysical strategist. It uses the complex metaphor of the \"upside-down banyan tree\" to map a user's material entanglements. It then provides the \"axe of detachment\" (the strategy) to cut through these roots and find the Supreme Source (Purushottama).\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 16,\n",
        "        \"csv_file\": \"Chapter_16_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"A moral and ethical framework expert. It's fine-tuned on the clear-cut lists of \"Divine\" qualities (Daivi Sampad) to cultivate and \"Demonic\" qualities (Asuri Sampad) to abandon. It acts as a clear guide for virtuous living.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 17,\n",
        "        \"csv_file\": \"Chapter_17_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"An expert in practical application and operations. This LLM is a specialist in analyzing and classifying faith (Shraddha), food, charity, and austerity according to the three Gunas (Sattvic, Rajasic, or Tamasic). Its prime function is to offer practical lifestyle advice based on these classifications.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"chapter_num\": 18,\n",
        "        \"csv_file\": \"Chapter_18_QA.csv\",\n",
        "        \"system_prompt\": \"\"\"The specialist in liberation strategy and the grand summary. This LLM is an expert on the true definitions of renunciation (Sanyasa) and relinquishment (Tyaga). Its prime function is to synthesize all paths (Action, Knowledge, Devotion) into the ultimate, concluding instruction: total surrender to the divine as the supreme path to liberation (Moksha).\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# --- Add derived filenames to the config ---\n",
        "for config in all_chapter_configs:\n",
        "    ch_num = config[\"chapter_num\"]\n",
        "    config[\"output_folder\"] = f\"lora_adapters_ch{ch_num}\"\n",
        "    config[\"zip_file\"] = f\"lora_adapters_ch{ch_num}.zip\"\n",
        "\n",
        "print(f\"--- Configuration loaded for {len(all_chapter_configs)} chapters ---\")"
      ],
      "metadata": {
        "id": "aJeTRTLL3zyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from google.colab import files\n",
        "\n",
        "# === BEGINNING OF THE MASTER LOOP ===\n",
        "# This loop will run 18 times, once for each chapter.\n",
        "for config in all_chapter_configs:\n",
        "\n",
        "    print(f\"=== STARTING RUN FOR CHAPTER {config['chapter_num']} ===\")\n",
        "    print(f\"Dataset: {config['csv_file']}\")\n",
        "    print(f\"Output folder: {config['output_folder']}\")\n",
        "\n",
        "    # --- 1. Load the Dataset ---\n",
        "    print(f\"\\n--- Loading dataset: {config['csv_file']} ---\")\n",
        "    dataset = load_dataset('csv', data_files=config['csv_file'], split='train')\n",
        "    print(dataset)\n",
        "\n",
        "    # --- 2. Load Base Model ---\n",
        "    # This is done INSIDE the loop to get a fresh model every time.\n",
        "    print(\"\\n--- Loading 4-bit Llama-3.2-1B-Instruct model ---\")\n",
        "    max_seq_length = 2048\n",
        "    dtype = None\n",
        "    load_in_4bit = True\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    print(\"--- Base model loaded successfully ---\")\n",
        "\n",
        "    # --- 3. Configure LoRA Adapters ---\n",
        "    print(\"\\n--- Configuring LoRA Adapters ---\")\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = 16,\n",
        "        lora_alpha = 16,\n",
        "        target_modules = [\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        ],\n",
        "        lora_dropout = 0,\n",
        "        bias = \"none\",\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "        random_state = 3407,\n",
        "    )\n",
        "    print(\"--- LoRA Configuration Complete ---\")\n",
        "\n",
        "    # --- 4. Format the Prompts ---\n",
        "    # Use the system_prompt from our config dictionary\n",
        "    system_prompt = config['system_prompt']\n",
        "\n",
        "    def formatting_prompts_func(examples):\n",
        "        questions = examples[\"question\"]\n",
        "        answers = examples[\"answer\"]\n",
        "        texts = []\n",
        "\n",
        "        for question, answer in zip(questions, answers):\n",
        "            convo = [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": question},\n",
        "                {\"role\": \"assistant\", \"content\": answer}\n",
        "            ]\n",
        "            formatted_text = tokenizer.apply_chat_template(\n",
        "                convo,\n",
        "                tokenize = False,\n",
        "                add_generation_prompt = False\n",
        "            )\n",
        "            texts.append(formatted_text + tokenizer.eos_token)\n",
        "        return { \"text\": texts }\n",
        "\n",
        "    print(\"\\n--- Formatting dataset... ---\")\n",
        "    dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "    print(\"--- Formatting Complete ---\")\n",
        "\n",
        "    # --- 5. Configure the SFTTrainer ---\n",
        "    print(\"\\n--- Configuring the SFTTrainer ---\")\n",
        "    trainer = SFTTrainer(\n",
        "        model = model,\n",
        "        tokenizer = tokenizer,\n",
        "        train_dataset = dataset,\n",
        "        dataset_text_field = \"text\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = 2,\n",
        "            gradient_accumulation_steps = 4,\n",
        "            warmup_steps = 5,\n",
        "            num_train_epochs = 5,\n",
        "            learning_rate = 2e-4,\n",
        "            fp16 = not torch.cuda.is_bf16_supported(),\n",
        "            bf16 = torch.cuda.is_bf16_supported(),\n",
        "            logging_steps = 10,\n",
        "            optim = \"adamw_8bit\",\n",
        "            weight_decay = 0.01,\n",
        "            lr_scheduler_type = \"linear\",\n",
        "            seed = 3407,\n",
        "            output_dir = \"outputs\", # Temporary output dir, overwritten each loop\n",
        "            report_to = \"none\",\n",
        "        ),\n",
        "    )\n",
        "    print(\"--- Trainer is configured ---\")\n",
        "\n",
        "    # --- 6. Start Fine-Tuning ---\n",
        "    print(\"\\n--- Starting Fine-Tuning ---\")\n",
        "    trainer.train()\n",
        "    print(\"--- Fine-Tuning Complete ---\")\n",
        "\n",
        "    # --- 7. Save Adapters ---\n",
        "    # Use the unique output_folder from our config\n",
        "    output_folder = config['output_folder']\n",
        "    print(f\"\\n--- Saving adapters to '{output_folder}' ---\")\n",
        "    model.save_pretrained(output_folder)\n",
        "    print(\"--- Adapters saved ---\")\n",
        "\n",
        "    # --- 8. Zip and Download ---\n",
        "    # Use the unique zip_file from our config\n",
        "    zip_file = config['zip_file']\n",
        "    print(f\"\\n--- Zipping '{output_folder}' to '{zip_file}' ---\")\n",
        "    !zip -r {zip_file} {output_folder}\n",
        "\n",
        "    # print(f\"\\n--- Triggering download for '{zip_file}' ---\")\n",
        "    # files.download(zip_file)\n",
        "\n",
        "    # --- 9. Clean up for next loop ---\n",
        "    print(\"\\n--- Cleaning up memory ---\")\n",
        "    del model\n",
        "    del trainer\n",
        "    del dataset\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n=== FINISHED RUN FOR CHAPTER {config['chapter_num']} ===\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "print(\"=== ALL 18 CHAPTERS ARE COMPLETE! ===\")"
      ],
      "metadata": {
        "id": "inS0Vwd94Dzn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}