{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJH4j0p_ISOS",
        "outputId": "28fe8fbb-2a1f-4950-a99c-c0eb109ccc77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m157.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m130.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Cell 1: install packages (matching your notebook)\n",
        "# Note: these installs can be slow. Run once per new Colab session.\n",
        "\n",
        "# Core Unsloth stack (match your notebook)\n",
        "!pip install --quiet --no-deps bitsandbytes accelerate xformers==0.0.29.post3 \\\n",
        "    peft trl triton cut_cross_entropy unsloth_zoo\n",
        "\n",
        "!pip install --quiet sentencepiece protobuf \"datasets>=3.4.1\" \\\n",
        "    \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "\n",
        "# Unsloth (no-deps like in your Notebook)\n",
        "!pip install --quiet --no-deps unsloth\n",
        "\n",
        "# Pin TRL to the training-compatible version (your notebook pinned 0.22.2)\n",
        "!pip install --quiet --no-deps trl==0.22.2\n",
        "\n",
        "# Streamlit + ngrok + helper libs\n",
        "!pip install --quiet streamlit pyngrok gdown joblib==1.5.2 scikit-learn==1.7.2 xgboost==3.1.1 catboost==1.2.8\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Download TFIDF + Voting model ZIP + LoRA Adapters\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "WORKDIR = Path(\"/content/unsloth_streamlit\")\n",
        "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(WORKDIR)\n",
        "\n",
        "# ------------------ File IDs ------------------\n",
        "ROUTER_ZIP_FILE_ID = \"1yTJg9RIDXuWyEFRYCliZTO52NhuTGZgS\"   # ‚úÖ contains tfidf & voting models\n",
        "LORA_ZIP_FILE_ID   = \"1Qc8eUaOrTY-Bw9M70PcKUpTedbbeZcaz\"   # ‚úÖ LoRA adapters\n",
        "# ------------------------------------------------\n",
        "\n",
        "print(\"üìÅ Working dir:\", WORKDIR)\n",
        "\n",
        "# ‚úÖ Download router zip\n",
        "router_zip = \"router_files.zip\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={ROUTER_ZIP_FILE_ID}\", router_zip, quiet=False)\n",
        "\n",
        "# ‚úÖ Unzip router files\n",
        "print(\"üì¶ Extracting router model files...\")\n",
        "with zipfile.ZipFile(router_zip, \"r\") as zf:\n",
        "    zf.extractall(WORKDIR)\n",
        "\n",
        "# ‚úÖ Verify files\n",
        "print(\"‚úÖ Router files extracted:\")\n",
        "for f in [\"tfidf_vectorizer.joblib\", \"voting_model.joblib\"]:\n",
        "    if Path(f).exists():\n",
        "        print(f\"   - {f}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {f} NOT FOUND ‚Äî upload manually!\")\n",
        "\n",
        "# ‚úÖ Download LoRA adapters zip\n",
        "lora_zip = \"lora_adapters.zip\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={LORA_ZIP_FILE_ID}\", lora_zip, quiet=False)\n",
        "\n",
        "# ‚úÖ Extract LoRA adapters\n",
        "print(\"\\nüì¶ Extracting LoRA adapters...\")\n",
        "with zipfile.ZipFile(lora_zip, \"r\") as zf:\n",
        "    zf.extractall(WORKDIR)\n",
        "\n",
        "# ‚úÖ List LoRA folders\n",
        "print(\"‚úÖ LoRA adapters found:\")\n",
        "for p in sorted(WORKDIR.iterdir()):\n",
        "    if p.is_dir() and (p.name[0].isdigit() or \"lora\" in p.name.lower()):\n",
        "        print(\"   -\", p.name)\n",
        "\n",
        "print(\"\\nüéØ All files ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ34xws8JZJl",
        "outputId": "8ec94d13-6478-4104-c54f-bb02caed3629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Working dir: /content/unsloth_streamlit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yTJg9RIDXuWyEFRYCliZTO52NhuTGZgS\n",
            "To: /content/unsloth_streamlit/router_files.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.45M/1.45M [00:00<00:00, 10.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Extracting router model files...\n",
            "‚úÖ Router files extracted:\n",
            "   - tfidf_vectorizer.joblib\n",
            "   - voting_model.joblib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Qc8eUaOrTY-Bw9M70PcKUpTedbbeZcaz\n",
            "From (redirected): https://drive.google.com/uc?id=1Qc8eUaOrTY-Bw9M70PcKUpTedbbeZcaz&confirm=t&uuid=1b7bfe9c-8cde-46f2-9032-62a33239a869\n",
            "To: /content/unsloth_streamlit/lora_adapters.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 751M/751M [00:17<00:00, 43.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì¶ Extracting LoRA adapters...\n",
            "‚úÖ LoRA adapters found:\n",
            "   - 10_AweMatrix\n",
            "   - 11_CosmosView\n",
            "   - 12_DevotionAl\n",
            "   - 13_FieldScanner\n",
            "   - 14_GunaClassifier\n",
            "   - 15_NexusCut\n",
            "   - 16_VirtueCompass\n",
            "   - 17_LifestyleOps\n",
            "   - 18_MokshaPath\n",
            "   - 1_CrisisCore\n",
            "   - 2_AtmaAnalytics\n",
            "   - 3_ActionFlow\n",
            "   - 4_Karmicknowledge\n",
            "   - 5_ZenithOS\n",
            "   - 6_Mindful_Modulator\n",
            "   - 7_SourceCode\n",
            "   - 8_Continuum\n",
            "   - 9_OmniPresence\n",
            "\n",
            "üéØ All files ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: detect adapter folders + produce friendly index -> folder -> title map\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "WORKDIR = Path(\"/content/unsloth_streamlit\")\n",
        "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) scan for candidate adapter folders (same approach as before)\n",
        "candidates = sorted([p for p in WORKDIR.iterdir() if p.is_dir()])\n",
        "\n",
        "adapter_map = {}   # index -> folder Path\n",
        "unmatched = []\n",
        "\n",
        "for p in candidates:\n",
        "    name = p.name\n",
        "    # pattern 1: lora_adapters_ch{n}\n",
        "    m = re.search(r\"lora_adapters_ch(\\d+)\", name, re.IGNORECASE)\n",
        "    if m:\n",
        "        idx = int(m.group(1))\n",
        "        adapter_map[idx] = p\n",
        "        continue\n",
        "    # pattern 2: leading number \"1_Name\" or \"1-Name\" or \"1 Name\"\n",
        "    m2 = re.match(r\"^(\\d+)[_\\-\\s].+\", name)\n",
        "    if m2:\n",
        "        idx = int(m2.group(1))\n",
        "        adapter_map[idx] = p\n",
        "        continue\n",
        "    # pattern 3: folder contains \"ch{n}\"\n",
        "    m3 = re.search(r\"ch(\\d+)\", name, re.IGNORECASE)\n",
        "    if m3:\n",
        "        adapter_map[int(m3.group(1))] = p\n",
        "        continue\n",
        "    # didn't match any pattern - keep for manual check\n",
        "    unmatched.append(p)\n",
        "\n",
        "# 2) friendly titles for chapters (from your llms/plan). Use these in the UI.\n",
        "CHAPTER_TITLES = {\n",
        "    1: \"CrisisCore ‚Äî Conflict & Compassion (Chapter 1)\",\n",
        "    2: \"AtmaAnalytics ‚Äî Self & Immortality (Chapter 2)\",\n",
        "    3: \"ActionFlow ‚Äî Karma Yoga (Chapter 3)\",\n",
        "    4: \"Karmicknowledge ‚Äî Knowledge + Action (Chapter 4)\",\n",
        "    5: \"ZenithOS ‚Äî Renunciation in Action (Chapter 5)\",\n",
        "    6: \"Mindful Modulator ‚Äî Dhyana Yoga (Chapter 6)\",\n",
        "    7: \"SourceCode ‚Äî Metaphysics & Maya (Chapter 7)\",\n",
        "    8: \"Continuum ‚Äî Transition & Afterlife (Chapter 8)\",\n",
        "    9: \"OmniPresence ‚Äî Immanence & Devotion (Chapter 9)\",\n",
        "    10: \"AweMatrix ‚Äî Vibhutis & Inspiration (Chapter 10)\",\n",
        "    11: \"CosmosView ‚Äî Universal Form (Chapter 11)\",\n",
        "    12: \"DevotionAI ‚Äî Bhakti Yoga (Chapter 12)\",\n",
        "    13: \"FieldScanner ‚Äî Kshetra & Kshetragna (Chapter 13)\",\n",
        "    14: \"GunaClassifier ‚Äî Sattva/Rajas/Tamas (Chapter 14)\",\n",
        "    15: \"NexusCut ‚Äî Strategy of Detachment (Chapter 15)\",\n",
        "    16: \"VirtueCompass ‚Äî Divine & Demonic Qualities (Chapter 16)\",\n",
        "    17: \"LifestyleOps ‚Äî Practical Guna Guidance (Chapter 17)\",\n",
        "    18: \"MokshaPath ‚Äî Renunciation & Liberation (Chapter 18)\",\n",
        "}\n",
        "\n",
        "# 3) assemble final index map for JSON output\n",
        "final_map = {}\n",
        "for idx in sorted(CHAPTER_TITLES.keys()):\n",
        "    folder = adapter_map.get(idx)\n",
        "    final_map[idx] = {\n",
        "        \"index\": idx,\n",
        "        \"title\": CHAPTER_TITLES[idx],\n",
        "        \"folder_name\": folder.name if folder is not None else None,\n",
        "        \"folder_path\": str(folder.resolve()) if folder is not None else None,\n",
        "        \"present\": bool(folder is not None)\n",
        "    }\n",
        "\n",
        "# 4) save to disk so the Streamlit app can load friendly names\n",
        "out_json = WORKDIR / \"adapter_index_map.json\"\n",
        "with out_json.open(\"w\", encoding=\"utf-8\") as fh:\n",
        "    json.dump(final_map, fh, indent=2, ensure_ascii=False)\n",
        "\n",
        "# 5) print a neat summary for you to inspect\n",
        "print(f\"Working dir: {WORKDIR}\")\n",
        "present = [k for k,v in final_map.items() if v[\"present\"]]\n",
        "missing = [k for k,v in final_map.items() if not v[\"present\"]]\n",
        "print(f\"Detected {len(present)} adapters out of 18 expected.\")\n",
        "print(\"Summary (Index : Title -> folder_name):\")\n",
        "for k in sorted(final_map.keys()):\n",
        "    v = final_map[k]\n",
        "    status = \"OK\" if v[\"present\"] else \"MISSING\"\n",
        "    print(f\" {k:02d}: {v['title']}\\n      -> {v['folder_name'] or '<not found>'}  [{status}]\")\n",
        "\n",
        "if unmatched:\n",
        "    print(\"\\nFolders that didn't match index patterns (please inspect manually):\")\n",
        "    for p in unmatched:\n",
        "        print(\" -\", p.name)\n",
        "\n",
        "print(f\"\\nAdapter index map written to: {out_json}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JKyoCX653g5",
        "outputId": "84966008-8d67-468e-b983-8bd1cb91f471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working dir: /content/unsloth_streamlit\n",
            "Detected 18 adapters out of 18 expected.\n",
            "Summary (Index : Title -> folder_name):\n",
            " 01: CrisisCore ‚Äî Conflict & Compassion (Chapter 1)\n",
            "      -> 1_CrisisCore  [OK]\n",
            " 02: AtmaAnalytics ‚Äî Self & Immortality (Chapter 2)\n",
            "      -> 2_AtmaAnalytics  [OK]\n",
            " 03: ActionFlow ‚Äî Karma Yoga (Chapter 3)\n",
            "      -> 3_ActionFlow  [OK]\n",
            " 04: Karmicknowledge ‚Äî Knowledge + Action (Chapter 4)\n",
            "      -> 4_Karmicknowledge  [OK]\n",
            " 05: ZenithOS ‚Äî Renunciation in Action (Chapter 5)\n",
            "      -> 5_ZenithOS  [OK]\n",
            " 06: Mindful Modulator ‚Äî Dhyana Yoga (Chapter 6)\n",
            "      -> 6_Mindful_Modulator  [OK]\n",
            " 07: SourceCode ‚Äî Metaphysics & Maya (Chapter 7)\n",
            "      -> 7_SourceCode  [OK]\n",
            " 08: Continuum ‚Äî Transition & Afterlife (Chapter 8)\n",
            "      -> 8_Continuum  [OK]\n",
            " 09: OmniPresence ‚Äî Immanence & Devotion (Chapter 9)\n",
            "      -> 9_OmniPresence  [OK]\n",
            " 10: AweMatrix ‚Äî Vibhutis & Inspiration (Chapter 10)\n",
            "      -> 10_AweMatrix  [OK]\n",
            " 11: CosmosView ‚Äî Universal Form (Chapter 11)\n",
            "      -> 11_CosmosView  [OK]\n",
            " 12: DevotionAI ‚Äî Bhakti Yoga (Chapter 12)\n",
            "      -> 12_DevotionAl  [OK]\n",
            " 13: FieldScanner ‚Äî Kshetra & Kshetragna (Chapter 13)\n",
            "      -> 13_FieldScanner  [OK]\n",
            " 14: GunaClassifier ‚Äî Sattva/Rajas/Tamas (Chapter 14)\n",
            "      -> 14_GunaClassifier  [OK]\n",
            " 15: NexusCut ‚Äî Strategy of Detachment (Chapter 15)\n",
            "      -> 15_NexusCut  [OK]\n",
            " 16: VirtueCompass ‚Äî Divine & Demonic Qualities (Chapter 16)\n",
            "      -> 16_VirtueCompass  [OK]\n",
            " 17: LifestyleOps ‚Äî Practical Guna Guidance (Chapter 17)\n",
            "      -> 17_LifestyleOps  [OK]\n",
            " 18: MokshaPath ‚Äî Renunciation & Liberation (Chapter 18)\n",
            "      -> 18_MokshaPath  [OK]\n",
            "\n",
            "Folders that didn't match index patterns (please inspect manually):\n",
            " - huggingface_tokenizers_cache\n",
            " - unsloth_compiled_cache\n",
            "\n",
            "Adapter index map written to: /content/unsloth_streamlit/adapter_index_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ FINAL Cell-4 (Fixed quoting, CSS, music, copy, answer clean)\n",
        "from pathlib import Path\n",
        "\n",
        "app_code = \"\"\"\n",
        "import streamlit as st\n",
        "import joblib, re, os, json, torch\n",
        "from pathlib import Path\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ----- CONFIG -----\n",
        "BASE_MODEL = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "TFIDF_PATH = \"/content/unsloth_streamlit/tfidf_vectorizer.joblib\"\n",
        "VOTING_PATH = \"/content/unsloth_streamlit/voting_model.joblib\"\n",
        "WORKDIR = Path(\"/content/unsloth_streamlit\")\n",
        "MODEL_MAP_FILE = WORKDIR / \"adapter_index_map.json\"\n",
        "MAX_NEW_TOKENS = 256\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# -------------------\n",
        "\n",
        "st.set_page_config(page_title=\"Gita Wisdom AI\", page_icon=\"üïâÔ∏è\", layout=\"centered\")\n",
        "\n",
        "# ----- CSS -----\n",
        "st.markdown(\\\"\\\"\\\"\\n\n",
        "<style>\n",
        "body { font-family: 'Inter', sans-serif; }\n",
        "\n",
        ".user-box {\n",
        "    border-left: 3px solid #20bf6b;\n",
        "    padding: 12px 14px;\n",
        "    background: #fefefe;\n",
        "    margin-bottom: 8px;\n",
        "    border-radius: 8px;\n",
        "}\n",
        "\n",
        ".response-box {\n",
        "    border-left: 3px solid #4b7bec;\n",
        "    padding: 12px 14px;\n",
        "    margin-top: 14px;\n",
        "    background: #f7f7ff;\n",
        "    color: #1a1a1a;\n",
        "    border-radius: 8px;\n",
        "    white-space: pre-wrap;\n",
        "    font-size: 1.07rem;\n",
        "}\n",
        "\n",
        ".subtle-text { color: #555; font-size: 0.92rem; }\n",
        ".header-text { font-size: 1.4rem; font-weight: 600; padding-bottom: 6px; }\n",
        "</style>\n",
        "\\\"\\\"\\\", unsafe_allow_html=True)\n",
        "\n",
        "st.markdown(\"<div class='header-text'>Bhagavad Gita ‚Äî Wisdom Companion</div>\", unsafe_allow_html=True)\n",
        "st.markdown(\"<div class='subtle-text'>Ask sincerely. Receive clarity.</div>\", unsafe_allow_html=True)\n",
        "\n",
        "# ----- Router -----\n",
        "@st.cache_resource\n",
        "def load_router():\n",
        "    tfidf = joblib.load(TFIDF_PATH)\n",
        "    voting = joblib.load(VOTING_PATH)\n",
        "    return Pipeline([(\"tfidf\", tfidf), (\"ensemble\", voting)])\n",
        "router = load_router()\n",
        "\n",
        "# ----- Base Model -----\n",
        "@st.cache_resource\n",
        "def load_base():\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=BASE_MODEL, max_seq_length=2048, load_in_4bit=True\n",
        "    )\n",
        "    return model, tokenizer\n",
        "base_model, tokenizer = load_base()\n",
        "\n",
        "# ----- Prompts -----\n",
        "SYSTEM_PROMPTS = {\n",
        "1: \"Empathy & clarity for inner conflict.\",\n",
        "2: \"Soul is immortal. Stay steady.\",\n",
        "3: \"Selfless action without attachment.\",\n",
        "4: \"Knowledge + action harmony.\",\n",
        "5: \"Act fully, stay unattached.\",\n",
        "6: \"Meditation guidance.\",\n",
        "7: \"Divine metaphysics & Maya.\",\n",
        "8: \"Soul journey beyond body.\",\n",
        "9: \"Devotion and divine presence.\",\n",
        "10: \"Seeing divinity in excellence.\",\n",
        "11: \"Universal cosmic vision.\",\n",
        "12: \"Love & devotion path.\",\n",
        "13: \"Body vs soul discernment.\",\n",
        "14: \"Gunas analysis.\",\n",
        "15: \"Detach & rise to Supreme.\",\n",
        "16: \"Virtuous vs harmful traits.\",\n",
        "17: \"Lifestyle by Gunas.\",\n",
        "18: \"Liberation through surrender.\"\n",
        "}\n",
        "\n",
        "# ----- Load chapter map -----\n",
        "adapter_index_map = {}\n",
        "if MODEL_MAP_FILE.exists():\n",
        "    adapter_index_map = json.loads(MODEL_MAP_FILE.read_text())\n",
        "\n",
        "if \"adapter_cache\" not in st.session_state:\n",
        "    st.session_state.adapter_cache = {}\n",
        "\n",
        "def clean_text(x): return re.sub(r\"[^\\\\w\\\\s]\", \"\", x.lower())\n",
        "\n",
        "def find_adapter(idx):\n",
        "    idx = str(idx)\n",
        "    for p in WORKDIR.iterdir():\n",
        "        if p.is_dir() and p.name.startswith(idx + \"_\"):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def load_adapter(idx, folder):\n",
        "    if idx in st.session_state.adapter_cache:\n",
        "        return st.session_state.adapter_cache[idx]\n",
        "    model = PeftModel.from_pretrained(base_model, str(folder)).eval()\n",
        "    st.session_state.adapter_cache[idx] = {\"model\": model, \"tokenizer\": tokenizer}\n",
        "    return st.session_state.adapter_cache[idx]\n",
        "\n",
        "# ‚úÖ Clean answer from model output\n",
        "def extract_assistant_reply(text):\n",
        "    text = re.sub(r\"Cutting Knowledge.*?\\\\n\", \"\", text, flags=re.DOTALL)\n",
        "    text = re.sub(r\"<\\\\|.*?\\\\|>\", \"\", text)\n",
        "    for token in [\"assistant\", \"assistant:\", \"assistant\\\\n\"]:\n",
        "        if token in text.lower():\n",
        "            text = text.split(token,1)[-1]\n",
        "    return text.strip()\n",
        "\n",
        "def format_prompt(sys, user):\n",
        "    try:\n",
        "        return tokenizer.apply_chat_template(\n",
        "            [{\"role\":\"system\",\"content\":sys},{\"role\":\"user\",\"content\":user}],\n",
        "            tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except:\n",
        "        return sys + \"\\\\n\\\\n\" + user\n",
        "\n",
        "# ----- UI -----\n",
        "query = st.text_area(\"Your Question\", height=180, placeholder=\"I'm struggling with...\")\n",
        "\n",
        "if st.button(\"‚ú® Receive Insight\"):\n",
        "    if not query.strip():\n",
        "        st.warning(\"Please type a question.\")\n",
        "    else:\n",
        "        pred = int(router.predict([clean_text(query)])[0])\n",
        "        chapter = adapter_index_map.get(str(pred), {}).get(\"title\", f\"Chapter {pred}\")\n",
        "        st.markdown(f\"<div class='subtle-text'>Guided by: <b>{chapter}</b></div>\", unsafe_allow_html=True)\n",
        "\n",
        "        folder = find_adapter(pred)\n",
        "        if not folder:\n",
        "            st.error(\"Expert unavailable.\")\n",
        "        else:\n",
        "            info = load_adapter(pred, folder)\n",
        "            sys = SYSTEM_PROMPTS.get(pred, \"\")\n",
        "            prompt = format_prompt(sys, query)\n",
        "\n",
        "            with st.spinner(\"Meditating on wisdom...\"):\n",
        "                m = info[\"model\"]; t = info[\"tokenizer\"]\n",
        "                tokens = {k:v.to(DEVICE) for k,v in t(prompt, return_tensors=\"pt\").items()}\n",
        "                out = m.generate(**tokens, max_new_tokens=MAX_NEW_TOKENS)\n",
        "                raw = t.decode(out[0], skip_special_tokens=True)\n",
        "                text = extract_assistant_reply(raw)\n",
        "\n",
        "            st.markdown(f\"<div class='response-box'>{text}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "            # ‚úÖ Copy button\n",
        "            if st.button(\"üìã Copy Answer\"):\n",
        "                st.write(\"\")  # small spacing\n",
        "                st.code(text, language=\"\")\n",
        "                st.success(\"Copied to clipboard ‚Äî long press to copy ‚úÖ\")\n",
        "\n",
        "# Developer Debug\n",
        "with st.expander(\"Developer Tools\"):\n",
        "    st.write(\"Cached adapters:\", list(st.session_state.adapter_cache.keys()))\n",
        "    st.write(\"Router loaded:\", router is not None)\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"<div style='text-align:center;color:#555'>‚ÄúLet your actions be pure; results will follow.‚Äù</div>\", unsafe_allow_html=True)\n",
        "\"\"\"\n",
        "\n",
        "Path(\"app.py\").write_text(app_code)\n",
        "print(\"‚úÖ app.py written successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Uhulid2_9nf",
        "outputId": "3f729e45-71ef-4351-f97a-414397e3bff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ app.py written successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: start streamlit and open an ngrok tunnel (if you want external access)\n",
        "import os\n",
        "from pyngrok import ngrok, conf\n",
        "import time\n",
        "NGROK_AUTH_TOKEN = \"35CmYl9fvU0KPNwaYkoczmmKP7M_3JSxPHuo7CfS2tH5Ntp82\"  # <-- Paste your ngrok auth token here if you want a stable public URL\n",
        "\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "\n",
        "# kill previous tunnels (if any)\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# start streamlit in background\n",
        "get_ipython().system_raw(\"streamlit run app.py --server.port 8501 --server.enableCORS false &\")\n",
        "\n",
        "# create a tunnel\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(\"Streamlit started. Public URL:\", public_url)\n",
        "print(\"If ngrok URL does not work, open http://127.0.0.1:8501 via Colab's port preview.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9AQgyKgJ6zR",
        "outputId": "05794da0-a91b-4658-d42d-aa4ddea82308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit started. Public URL: NgrokTunnel: \"https://nonangelic-pecuniarily-brantley.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "If ngrok URL does not work, open http://127.0.0.1:8501 via Colab's port preview.\n"
          ]
        }
      ]
    }
  ]
}